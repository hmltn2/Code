"""
Literate Programming Example: N-Grams

This code demonstrates the process of generating and analyzing n-grams in a given text. It serves as a simple,
pedagogical example for understanding basic natural language processing techniques.
"""

# Step 1: Load a sample text
text = """
Winnie the Pooh, also called Pooh Bear, is a fictional anthropomorphic teddy bear created by A. A. Milne.
The first collection of stories about the character was the book Winnie-the-Pooh, and this was followed by The House at Pooh Corner.
"""

# Step 2: Tokenize the text on whitespace
tokens = text.split()

# Step 3: Define a function to generate n-grams
def generate_ngrams(tokens, n):
    """
    Generates n-grams from the given list of tokens.
    
    Args:
        tokens (list of str): The list of tokens.
        n (int): The length of the n-grams to generate.

    Returns:
        list of str: The generated n-grams.
    """
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]

# Step 4: Generate bigrams (n=2) and 5-grams (n=5)
bigrams = generate_ngrams(tokens, 2)
fivegrams = generate_ngrams(tokens, 5)

# Step 5: Define a function to count n-gram occurrences
def count_ngrams(ngrams):
    """
    Counts the occurrences of n-grams in the given list.
    
    Args:
        ngrams (list of str): The list of n-grams.

    Returns:
        dict: A dictionary with n-grams as keys and their counts as values.
    """
    ngram_counts = {}
    for ngram in ngrams:
        if ngram not in ngram_counts:
            ngram_counts[ngram] = 1
        else:
            ngram_counts[ngram] += 1

    return ngram_counts

# Step 6: Count bigram and 5-gram occurrences
bigram_counts = count_ngrams(bigrams)
fivegram_counts = count_ngrams(fivegrams)

# Step 7: Define a function to display the top n-grams
def display_top_ngrams(ngram_counts, n):
    """
    Displays the top n n-grams by frequency.
    
    Args:
        ngram_counts (dict): A dictionary with n-grams as keys and their counts as values.
        n (int): The number of top n-grams to display.
    """
    sorted_ngrams = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)

    for ngram, count in sorted_ngrams[:n]:
        print(f"{ngram}: {count}")

# Step 8: Display the top bigrams and 5-grams
print("Top bigrams:")
display_top_ngrams(bigram_counts, 10)

print("\nTop 5-grams:")
display_top_ngrams(fivegram_counts, 10)



"""
Continuation: Set Intersections and Topology in N-Grams

This section of the code introduces the concept of set intersections and their connection to topology. By examining the
intersections of the generated 5-grams, we can identify and rank salient n-grams (2, 3, and 4-grams) that are subsets
of the 5-grams.
"""

# Step 9: Define a function to find the set intersections of n-grams
def find_ngram_intersections(ngrams, min_n, max_n):
    """
    Finds the set intersections of n-grams, identifying smaller n-grams that are subsets of the input n-grams.
    
    Args:
        ngrams (list of str): The list of n-grams to analyze.
        min_n (int): The minimum length of the n-grams to generate from the intersections.
        max_n (int): The maximum length of the n-grams to generate from the intersections.

    Returns:
        dict: A dictionary with n-grams as keys and their counts as values.
    """
    intersections = {}

    for ngram in ngrams:
        tokens = ngram.split()

        for n in range(min_n, max_n + 1):
            sub_ngrams = generate_ngrams(tokens, n)

            for sub_ngram in sub_ngrams:
                if sub_ngram not in intersections:
                    intersections[sub_ngram] = 1
                else:
                    intersections[sub_ngram] += 1

    return intersections

# Step 10: Find intersections of the 5-grams (generating 2, 3, and 4-grams)
ngram_intersections = find_ngram_intersections(fivegrams, 2, 4)

# Step 11: Display the top n-grams from the intersections
print("Top n-grams from intersections:")
display_top_ngrams(ngram_intersections, 10)

"""
Topology and Linguistics

Topology is the study of properties that are preserved under continuous deformations, such as stretching and bending.
In the context of computational linguistics, topology can be related to the analysis of n-grams and the relationships
between different n-grams when considering set intersections and unions.

In this code, we find the set intersections of 5-grams to identify smaller n-grams (2, 3, and 4-grams) that are subsets
of the 5-grams. This approach highlights the presence of salient n-grams in the text, revealing patterns and structure
in the language. By considering the intersections and unions of n-grams, we explore the underlying topological
properties of the text, which can be valuable in a wide range of natural language processing tasks.

Expected Output:

The output frequency tables will show the top n-grams found in the input text, with a focus on salient n-grams
identified through set intersections. These n-grams are likely to be related to the most relevant or frequent
concepts in the text. The output should display ranked n-grams, along with their frequency counts, for bigrams,
5-grams, and the n-grams derived from set intersections.
"""

# Step 12: Output a sample table to illustrate the expected results

"""
Sample Output Frequency Tables:

These tables provide an example of what the output frequency tables might look like. They are not accurate and are
provided for illustration purposes only.

Top 10 Bigrams:
1. ('the', 'hundred') - 4
2. ('hundred', 'acre') - 4
3. ('acre', 'wood') - 4
4. ('winnie', 'the') - 3
5. ('the', 'pooh') - 3
6. ('and', 'piglet') - 3
7. ('rabbit', 'house') - 2
8. ('tigger', 'bounce') - 2
9. ('eeyore', 'tail') - 2
10. ('owl', 'wisdom') - 1

Top 10 5-Grams:
1. ('winnie the pooh and piglet') - 2
2. ('the hundred acre wood story') - 2
3. ('rabbit house and eeyore tail') - 1
4. ('tigger bounce and owl wisdom') - 1
5. ('a walk in the hundred') - 1
6. ('pooh and piglet visit rabbit') - 1
7. ('hundred acre wood with friends') - 1
8. ('the adventures of winnie the') - 1
9. ('eeyore loses his tail again') - 1
10. ('owl shares wisdom with pooh') - 1

Top 10 n-Grams from Intersections:
1. ('the hundred acre') - 8
2. ('winnie the pooh') - 6
3. ('and piglet') - 4
4. ('rabbit house') - 4
5. ('tigger bounce') - 4
6. ('eeyore tail') - 4
7. ('owl wisdom') - 2
8. ('hundred acre wood') - 2
9. ('the hundred') - 2
10. ('pooh and piglet') - 2
"""




# Final Addenda:
#
# 1. What is often called machine learning is a subset of a way more general, and simple, idea.
#    It is actually basically functional analysis, because it is about transformations on functions.
#    The real question, when you try to apply it, is, when you want something to “learn” something,
#    you are arguably (most abstractly) wanting nothing more than a transformation. Sometimes you know
#    the exact output desired, for the transformation. Sometimes you only have a criteria you want it
#    to fulfill; you don’t know what the object looks like that fulfills it; you do not even know if
#    it exists (mathematicians often work on questions of the existence or non existence of a particular
#    object). The idea of being inside a set where a particular transformation may produce some other
#    object (presumably of the same fundamental kind, and hence, likely in the same set) brings us
#    immediately into abstract algebra, and also category theory. When we say “learning”, we really just
#    want to know if there’s a “path” - in algebraic terms I suppose like, a homology or something,
#    an existing sequence of functions to get you there? “Path” sounds like point set topology - but I
#    believe they are related, anyhow! - to get from a to b (where b is a set of criteria - either only
#    few, or so strong to determine some object up to uniqueness, but it’s actually no longer the
#    important part, the more and more abstract we become —..).
#
# 2. What that means is arguably that we can start from a clean start, fresh slate, to not dive straight
#    into neural networks. Instead, it is better to consider any real world phenomenon - a mystifying
#    social trend which you want to know if its a pattern - music - language - you want to know if some
#    “object” under some transformation could ever take on / mirror the form of that pattern (ie, to
#    understand language must mean in a way to have the actual parsing algorithms of language encoded
#    into you, somehow, somewhere). Again, the idea of emulating a pattern is no longer distinctive or
#    the point. It might be a mere criteria, not full capable learning (even modern ML does not fully
#    learn things, LLMs do not fully understand language, haven’t mastered it like a human). Now we only
#    want to know if a system has a valid sequence of transformations to arrive at a different state.
#    This is actually like Lagrangian mechanics where we can calculate a number of the possible physical
#    states a system can be in to kind of calculate its overall entropy / energy / information density,
#    and compute how like it is to spontaneously become some other form or state, become something else.)
#    ULTIMATELY we come full circle back to trying to make algorithms that can copy language patterns,
#    any at all… starting at the beginning.. the simplest pattern could be a “dumb” algorithm who, the
#    first word it is given, remembers the second, and that’s it. It only has been modified to say a
#    word, after a particular word to trigger it. Go forward from there. Start with very very simple
#    algorithms. Try doing n-grams on SYMBOLS - see what you find. Try doing n-grams MULTI-SCALE -
#    averaged over all context windows. Go from there.
#
# 3. The real (
#    hard, interesting, broad) purpose of this was to try to find multi word expressions, words or
#    phrases that are unique and determining, not just common. Evaluate if our n gram method was good
#    at that. If not, how could we change it? The intersection of all the n grams finds us a word that
#    is not only common but highly distributed. But that’s not quite the same… I think we can use
#    simple n gram techniques, basic discrete groupings, to learn so much… what we want to see is if
#    there’s a word that’s kind of *consistent* about who it hangs out with - like, a word that is not
#    distributed at all, but focalized on only a few words. That could imply it determines them. It’s
#    conditional logic: GIVEN A, B has a very high probability! So the final calculation is simple!
#    Calculate the conditional probability of every word relative to a 2 gram or all n grams! Now sum up
#    which word had the highest total of conditional probabilities - that is a word that, when it comes
#    knocking, you know it has brought a LOT on its tail. It implies the most.
#

#    The main purpose of this exploration was to find multi-word expressions or phrases that are
#    unique and determining, not just common. Evaluate if our n-gram method was effective in this
#    regard. If not, how could we improve it? The intersection of all the n-grams finds us a word that
#    is not only common but also highly distributed. But that’s not quite the same… We can use
#    simple n-gram techniques, basic discrete groupings, to learn a great deal. What we want to identify
#    is if there’s a word that is *consistent* about which words it co-occurs with - like, a word that is not
#    distributed at all, but focalized on only a few words. That could imply it determines them. It's
#    related to conditional probability: GIVEN A, B has a very high probability! So, the final calculation
#    is simple! Calculate the conditional probability of every word relative to a 2-gram or all n-grams! Now sum up
#    which word had the highest total of conditional probabilities - that is a word that, when it appears,
#    you know it has brought a LOT in its context. It implies the most.
#
#    This idea is related to the field of distributional semantics, which posits that words with similar
#    meanings tend to occur in similar contexts. By examining the distribution of words and their co-occurrence
#    patterns, we can identify relationships between them and better understand the underlying structure
#    of the text.



Suggested Readings:

1. Jurafsky, D., & Martin, J. H. (2019). Speech and Language Processing (3rd Edition). Prentice Hall.

2. Manning, C. D., & Schütze, H. (1999). Foundations of Statistical Natural Language Processing. MIT Press.

3. Hatcher, A. (2002). Algebraic Topology. Cambridge University Press.

4. Awodey, S. (2010). Category Theory (2nd Edition). Oxford University Press.

5. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.

'''

Firth, J. R. (1957). "A Synopsis of Linguistic Theory 1930-1955." Studies in Linguistic Analysis, pp. 1-32.
This influential work by J.R. Firth discusses the distributional hypothesis, which posits that words that occur in the same contexts tend to have similar meanings. It serves as a foundation for much of the work in distributional semantics and co-occurrence analysis.



# Other Suggested Readings:
# 1. Jurafsky, D., & Martin, J. H. (2019). Speech and Language Processing (3rd ed.). https://web.stanford.edu/~jurafsky/slp3/
# 2. Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.
# 3. Harris, Z. S. (1954). Distributional structure. Word, 10(2-3), 146-162.
# 4. Turney, P. D., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37, 141-188.
# 5. Baroni, M., Dinu, G., & Kruszewski, G. (2014). Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 238-247.



























